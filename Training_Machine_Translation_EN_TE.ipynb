{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit-Potnuru/Machine-Translation-EN-TE/blob/main/Training_Machine_Translation_EN_TE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83U_EjIAf-QX",
        "outputId": "670a4f20-fd25-4918-8c49-ca68c2c0e488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04F2mIUHJeDB"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2bRuyRlYJiOp"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def format_time(seconds):\n",
        "    hours = seconds // 3600\n",
        "    minutes = (seconds % 3600) // 60\n",
        "    seconds = seconds % 60\n",
        "\n",
        "    if hours > 0:\n",
        "        return f\"{int(hours):02d}::{int(minutes):02d}::{int(seconds):02d}\"\n",
        "    elif minutes > 0:\n",
        "        return f\"{int(minutes):02d}::{int(seconds):02d}\"\n",
        "    else:\n",
        "        return f\"{seconds:.3f} sec\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jsvKEkzgJqT4"
      },
      "outputs": [],
      "source": [
        "def num_parameters(n_parameter):\n",
        "  scales = {1e12: \"T\",\n",
        "            1e9: \"B\",\n",
        "            1e6: \"M\",\n",
        "            1e3: \"K\"\n",
        "           }\n",
        "  for scale, val in scales.items():\n",
        "    if(n_parameter > scale):\n",
        "      return f'{n_parameter/scale}{val} parameters'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyBXS0rY7VYS"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory-profiler\n",
        "%load_ext memory_profiler"
      ],
      "metadata": {
        "id": "k5zzqKTa9jw3",
        "outputId": "dd5285af-227e-43f1-f7c5-86a21a3a1a25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: memory-profiler in /usr/local/lib/python3.10/dist-packages (0.61.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler) (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osPIT_7rr5qT"
      },
      "source": [
        "## English and Telugu Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dKPeH1sQFMc8"
      },
      "outputs": [],
      "source": [
        "# Start the timer\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4Edn75HChFJ",
        "outputId": "f7bd6688-709a-4079-9b1c-07e132299ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of characters for Telugu Vocbulary is 123\n",
            "total number of characters for English Vocbulary is 72\n"
          ]
        }
      ],
      "source": [
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "END_TOKEN = '<END>'\n",
        "telugu_vocabulary = [START_TOKEN,\n",
        "    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "    ':', '<', '=', '>', '?', '@', 'ˌ',\n",
        "    '[', '\\\\', ']', '^', '_', '`','{', '|', '}', '~',\n",
        "\n",
        "    'అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ౠ', 'ఎ', 'ఏ', 'ఐ', 'ఒ','ఓ', 'ఔ',\n",
        "    'ా', 'ి', 'ీ', 'ు', 'ూ', 'ృ', 'ౄ', 'ె', 'ే',  'ై',  'ొ', 'ో', 'ౌ', '్',  'ఁ', 'ం', 'ః','ఀ',\n",
        "\n",
        "    'క', 'ఖ', 'గ', 'ఘ', 'ఙ',\n",
        "    'చ', 'ఛ', 'జ', 'ఝ','ఞ',\n",
        "    'ట', 'ఠ', 'డ', 'ఢ', 'ణ',\n",
        "    'త', 'థ', 'ద', 'ధ', 'న',\n",
        "    'ప', 'ఫ', 'బ', 'భ', 'మ',\n",
        "    'య', 'ర', 'ల', 'వ', 'ళ', 'శ', 'ష', 'స', 'హ', 'ఱ',\n",
        "\n",
        "   # 0,   1,   2,   3,   4,   5,   6,   7,   8,  9\n",
        "    '౦', '౧', '౨', '౩', '౪', '౫', '౬', '౭', '౮', '౯',\n",
        "    PADDING_TOKEN, END_TOKEN\n",
        "]\n",
        "\n",
        "english_vocabulary = [START_TOKEN,\n",
        "    ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "    ':', '<', '=', '>', '?', '@', 'ˌ',\n",
        "    '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~',\n",
        "\n",
        "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "    'y', 'z',\n",
        "    PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "print(f'total number of characters for Telugu Vocbulary is {len(telugu_vocabulary)}')\n",
        "print(f'total number of characters for English Vocbulary is {len(english_vocabulary)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5vCO_4OnCuNB"
      },
      "outputs": [],
      "source": [
        "def generate_index_key_map(keys):\n",
        "  key_to_index = { ch: i for i, ch in enumerate(keys)}\n",
        "  index_to_key = { i: ch for ch, i in key_to_index.items()}\n",
        "  return key_to_index, index_to_key\n",
        "\n",
        "telugu_to_index, index_to_telugu = generate_index_key_map(telugu_vocabulary)\n",
        "english_to_index, index_to_english = generate_index_key_map(english_vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DOb-HrQXD-we"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence, key_to_index, max_sentence_length, start = True, end = True):\n",
        "  sentence_indicies = []\n",
        "  if start:\n",
        "    sentence_indicies += [key_to_index[START_TOKEN]]\n",
        "  sentence_indicies += [key_to_index[ch] for ch in list(sentence)]\n",
        "  if end:\n",
        "    sentence_indicies.append(key_to_index[END_TOKEN])\n",
        "  # Adding PADDING_TOKEN so that sentence_indicies array length is equivalent to max_sentence_length\n",
        "  if max_sentence_length is not None:\n",
        "    sentence_indicies += [key_to_index[PADDING_TOKEN]] * (max_sentence_length - len(sentence_indicies))\n",
        "  return sentence_indicies\n",
        "\n",
        "def replace_token(ch, replaceTokens):\n",
        "  if ch in replaceTokens:\n",
        "    return replaceTokens[ch]\n",
        "  return ch\n",
        "\n",
        "\n",
        "def generate_encoder_decoder(key_to_index, index_to_key):\n",
        "  replaceTokens = {START_TOKEN : \"\", PADDING_TOKEN : \"\", END_TOKEN : \"\"}\n",
        "\n",
        "  encoder = lambda s, max_sentence_length = None, start = True, end = True: tokenize(s.lower(), key_to_index, max_sentence_length, start, end)\n",
        "  decoder = lambda enc_list, replaceTokens = replaceTokens: ''.join([replace_token(index_to_key[i], replaceTokens) for i in enc_list])\n",
        "\n",
        "  return encoder, decoder\n",
        "\n",
        "telugu_sen_encoder, telugu_sen_decoder = generate_encoder_decoder(telugu_to_index, index_to_telugu)\n",
        "english_sen_encoder, english_sen_decoder = generate_encoder_decoder(english_to_index, index_to_english)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lJgs4AqM4Xr7"
      },
      "outputs": [],
      "source": [
        "# def generate_encoder_decoder(key_to_index, index_to_key, enable_sentence_list = False):\n",
        "#   replaceTokens = {START_TOKEN : \"\", PADDING_TOKEN : \"\", END_TOKEN : \"\"}\n",
        "\n",
        "#   if (enable_sentence_list):\n",
        "#     encoder = lambda list_s, max_sentence_length: \\\n",
        "#                                 [tokenize(s.lower(), index_to_key, max_sentence_length) for s in list_s]\n",
        "#     decoder = lambda enc_sen_list, replaceTokens = replaceTokens: \\\n",
        "#                                 [''.join([replace_token(index_to_key[i], replaceTokens) for i in enc_list]) for enc_list in enc_sen_list]\n",
        "#   else:\n",
        "#     encoder = lambda s, max_sentence_length: [tokenize(s.lower(), index_to_key, max_sentence_length) for key in s.lower()]\n",
        "#     decoder = lambda enc_list, replaceTokens = replaceTokens: ''.join([replace_token(index_to_key[i], replaceTokens) for i in enc_list])\n",
        "#   return encoder, decoder\n",
        "\n",
        "# def encoder(list_sen, max_sentence_length = None):\n",
        "#   sen_flag = [False]\n",
        "#   if(isinstance(list_sen, str)):\n",
        "#     list_sen = [list_sen]\n",
        "#     sen_flag[0] = True\n",
        "\n",
        "#   enc_list_sen = []\n",
        "#   for s in list_sen:\n",
        "#     enc_list_sen.append(tokenize(s.lower(), key_to_index, max_sentence_length))\n",
        "\n",
        "#   if(sen_flag[0]):\n",
        "#     return enc_list_sen[0]\n",
        "#   return enc_list_sen\n",
        "\n",
        "# def decoder(enc_list, replaceTokens = replaceTokens):\n",
        "#   sen_flag = [False]\n",
        "#   if(isinstance(enc_list[0], int)):\n",
        "#     enc_list = [enc_list]\n",
        "#     sen_flag[0] = True\n",
        "#   dec_list = []\n",
        "#   for enc_sen in enc_list:\n",
        "#     dec_sen = \"\"\n",
        "#     for i in enc_sen:\n",
        "#       dec_sen += replace_token(index_to_key[i], replaceTokens)\n",
        "#     dec_list.append(dec_sen)\n",
        "\n",
        "#   if(sen_flag[0]):\n",
        "#     return dec_list[0]\n",
        "#   return dec_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqckD7KmQ1pM",
        "outputId": "7fcdb197-5584-4b4f-ac7b-ce79b641b836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 51, 52, 1, 52, 1, 44, 56, 1, 47, 58, 57, 1, 47, 48, 48, 57, 64, 71]\n",
            "hi i am don seenu\n"
          ]
        }
      ],
      "source": [
        "print(english_sen_encoder('hi i am don deenu'))\n",
        "print(english_sen_decoder(english_sen_encoder('hi I am Don Seenu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpbbzOXCE-LY",
        "outputId": "c2155142-80f8-4cc7-85ef-fea338273514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time execution: 0.593 sec\n"
          ]
        }
      ],
      "source": [
        "print(f'time execution: {format_time(time.time() - start_time)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc18Fd0bLdwv"
      },
      "source": [
        "## Reading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7XBuG4_oFUQP"
      },
      "outputs": [],
      "source": [
        "# Start the timer\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sKgxNGyGLjv3"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/Machine Translation EN-TE/Dataset/filtered'\n",
        "english_file = f'{file_path}/train.en'\n",
        "telugu_file = f'{file_path}/train.te'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUY8KRbHMByL",
        "outputId": "cee6833f-97fa-4e50-8a16-3edf08dcfc43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sentences: 3756786\n"
          ]
        }
      ],
      "source": [
        "with open(english_file) as file:\n",
        "  english_sentences = [line.rstrip('\\n').lower() for line in file.readlines()]\n",
        "\n",
        "with open(telugu_file) as file:\n",
        "  telugu_sentences = [line.rstrip('\\n') for line in file.readlines()]\n",
        "\n",
        "assert len(telugu_sentences) == len(english_sentences), f\"English and Telugu sentences count are not same\"\n",
        "n = len(english_sentences)\n",
        "print(f'Total number of sentences: {n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6dSGBBVY55F"
      },
      "outputs": [],
      "source": [
        "# n = len(telugu_sentences)\n",
        "# te_len = [len(x) for x in telugu_sentences]\n",
        "# te_per = [np.percentile(te_len, i) for i in range(101)]\n",
        "\n",
        "# en_len = [len(x) for x in english_sentences]\n",
        "# en_per = [np.percentile(en_len, i) for i in range(101)]\n",
        "\n",
        "# # for idx, (e,t) in enumerate(zip(te_per, en_per)):\n",
        "# #   print(idx, e, t)\n",
        "\n",
        "# seq_len = 200\n",
        "# print(len([x for x in en_len if x > seq_len]))\n",
        "# print(len([x for x in te_len if x > seq_len]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEVzAr-FYgJ7",
        "outputId": "c29075d5-7f0a-4b83-8ad7-fd70a908e739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99th percentile length Telugu: 193.0\n",
            "99th percentile length English: 210.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 99\n",
        "print( f\"{PERCENTILE}th percentile length Telugu: {np.percentile([len(list(x)) for x in telugu_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(list(x)) for x in english_sentences], PERCENTILE)}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aorcIt2hA6y"
      },
      "outputs": [],
      "source": [
        "max_sequence_length = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9vNdoRzcWAK",
        "outputId": "e2db4669-d042-4f9c-a177-5a76900ce49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of VALID EN-TE sentence pairs : 3699089\n",
            "Total number of INVALID EN-TE sentence pairs : 57697\n"
          ]
        }
      ],
      "source": [
        "#Filtering sentences based on vocabulary characters\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "en_voc = set(english_vocabulary)\n",
        "te_voc = set(telugu_vocabulary)\n",
        "valid_sentence_indicies = []\n",
        "\n",
        "for i in range(len(english_sentences)):\n",
        "  en_sen, te_sen = english_sentences[i], telugu_sentences[i]\n",
        "  if (is_valid_length(en_sen, max_sequence_length) \\\n",
        "      and is_valid_length(te_sen, max_sequence_length) \\\n",
        "      and is_valid_tokens(en_sen, en_voc)) \\\n",
        "      and is_valid_tokens(te_sen, te_voc):\n",
        "        valid_sentence_indicies.append(i)\n",
        "\n",
        "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]\n",
        "telugu_sentences = [telugu_sentences[i] for i in valid_sentence_indicies]\n",
        "\n",
        "print(f'Total number of VALID EN-TE sentence pairs : {len(english_sentences)}')\n",
        "if (n - len(english_sentences)) > 0:\n",
        "  print(f'Total number of INVALID EN-TE sentence pairs : {n - len(english_sentences)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1lgZatzMMYd",
        "outputId": "3bf20ca6-a91b-4579-e9e6-a0ea9c94ac1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some Examples: \n",
            "have you heard about foie gras? -> ఇక ఫ్రూట్ ఫ్లైస్ గురించి మీరు విన్నారా?\n",
            "i never thought of acting in films. -> సూర్య సినిమాల్లో నటించాలని ఎప్పుడూ అనుకోలేదు.\n",
            "a case has been registered under sections 302 and 376, ipc. -> నిందితులపై సెక్షన్ 376 మరియు 302ల కింద కేసు నమోదు చేశాం.\n",
            "of this, 10 people succumbed to the injuries. -> అందులో 10 మంది తీవ్రంగా గాయపడ్డారు.\n"
          ]
        }
      ],
      "source": [
        "print(f'Some Examples: ')\n",
        "for en_sen, te_sen in zip(english_sentences[:4], telugu_sentences[:4]):\n",
        "  print(f'{en_sen} -> {te_sen}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfIkF0SKFaFs",
        "outputId": "fc1a63d4-56c6-4c61-cc7d-f004afa04012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time execution: 57.053 sec\n"
          ]
        }
      ],
      "source": [
        "print(f'time execution: {format_time(time.time() - start_time)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOudE0GyESR7"
      },
      "source": [
        "## Dataset Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIrp_sP7FV1e"
      },
      "outputs": [],
      "source": [
        "# Start the timer\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrKqWiMwDvjC"
      },
      "outputs": [],
      "source": [
        "# enc_english_sentences = english_sen_encoder(english_sentences, max_sequence_length)\n",
        "# del english_sentences\n",
        "# enc_telugu_sentences = telugu_sen_encoder(telugu_sentences, max_sequence_length)\n",
        "# del telugu_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAXxaA6h3Qwb"
      },
      "outputs": [],
      "source": [
        "def split_dataset(lang1_sentences, lang2_sentences, ratios):\n",
        "  assert sum(ratios) == 100, f'split is not perfect'\n",
        "  train_ratio, val_ratio, test_ratio = ratios\n",
        "  n = len(english_sentences)\n",
        "  n1, n2 = int(n * 0.01 * train_ratio), int(n * 0.01 * (train_ratio + val_ratio))\n",
        "\n",
        "  dataset = {}\n",
        "  dataset['train'] = (lang1_sentences[:n1], lang2_sentences[:n1])\n",
        "  dataset['val'] = (lang1_sentences[n1:n2], lang2_sentences[n1:n2])\n",
        "  dataset['test'] = (lang1_sentences[n2:], lang2_sentences[n2:])\n",
        "  return dataset\n",
        "\n",
        "en_te_dataset = split_dataset(english_sentences, telugu_sentences, [85, 7, 8])\n",
        "del english_sentences\n",
        "del telugu_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPxc2Cc-gOr6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Dataset Ref: https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, lang1_sentences,\n",
        "                       lang2_sentences,\n",
        "                       lang1_encoder,\n",
        "                       lang2_encoder,\n",
        "                       max_sentence_length):\n",
        "        self.lang1_sentences = lang1_sentences\n",
        "        self.lang2_sentences = lang2_sentences\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lang2_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.lang1_sentences[idx], self.lang2_sentences[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZjHmc0ujACz"
      },
      "outputs": [],
      "source": [
        "for split in en_te_dataset.keys():\n",
        "  en_te_dataset[split] = TextDataset(en_te_dataset[split][0],\n",
        "                                      en_te_dataset[split][1],\n",
        "                                      english_sen_encoder,\n",
        "                                      telugu_sen_encoder,\n",
        "                                      max_sequence_length\n",
        "                                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U3cvgheUGog",
        "outputId": "d62542c4-52e4-4094-d9e2-13b8b7080068"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3144225, 258936, 295928)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "len(en_te_dataset['train']), len(en_te_dataset['val']), len(en_te_dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY1QA25XuK3k"
      },
      "outputs": [],
      "source": [
        "# batch_size = 300000\n",
        "# trainDataLoader = DataLoader(train_dataset, batch_size)\n",
        "# iterator = iter(trainDataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgxaonTHujaR"
      },
      "outputs": [],
      "source": [
        "# start_time = time.time()\n",
        "# for batch_num, batch in enumerate(iterator):\n",
        "#   (en_batch, te_batch), tar_te_batch = batch\n",
        "#   print(batch_num)\n",
        "# print(f'time execution: {format_time(time.time() - start_time)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahyo304BFb0r",
        "outputId": "b6058865-29ff-4a40-9986-11d31d472077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time execution: 2.739 sec\n"
          ]
        }
      ],
      "source": [
        "print(f'time execution: {format_time(time.time() - start_time)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D0teYZfn1nM"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S74pnFZvCAI9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import io\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh0qMaDRK1DF"
      },
      "source": [
        "## Transformer Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0xjAUFm8dbH"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, config, mask):\n",
        "    super().__init__()\n",
        "    assert isinstance(mask, bool), f'mask should be boolean, please provide a valid \"mask\" input'\n",
        "\n",
        "    self.config = config\n",
        "    self.mask = mask\n",
        "    self.query = nn.Linear(config.n_embd, config.head_size, bias=False)\n",
        "    self.key = nn.Linear(config.n_embd, config.head_size)\n",
        "    self.value = nn.Linear(config.n_embd, config.head_size)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    B, T, C = q.shape\n",
        "    q = self.query(q) # (B, T, head_size)\n",
        "    k = self.key(k) # (B, T, head_size)\n",
        "    w = q @ k.transpose(-2, -1) * k.shape[-1] ** (-0.5) # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "    if (self.mask):\n",
        "      w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    w = F.softmax(w, dim = -1)\n",
        "    w = self.dropout(w)\n",
        "\n",
        "    v = self.value(v) # (B, T, head_size)\n",
        "    out = w @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "    return out # (B, T, head_size)\n",
        "\n",
        "class MultiAttentionHead(nn.Module):\n",
        "  def __init__(self, config, mask = False):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0, f'\"n_head\" should be divisible of \"n_embd\", please provide valid hyperparameter inputs'\n",
        "    assert config.n_embd == config.n_head * config.head_size, f'\"n_embd\" is not equal to \"n_head * config.head_size\", please provide valid hyperparameter inputs'\n",
        "\n",
        "    self.config = config\n",
        "    self.attn_heads = nn.ModuleList([AttentionHead(config, mask) for _ in range(config.n_head)])\n",
        "    self.proj = nn.Linear(config.n_head * config.head_size, config.n_embd)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self, q, k, v):\n",
        "    out = torch.cat([h(q, k, v)for h in self.attn_heads], dim = -1) # (B, T, n_head * head_size)\n",
        "    out = self.proj(out) # (B, T, n_embd)\n",
        "    out = self.dropout(out)\n",
        "    return out # (B, T, n_embd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcvWvPvThDoG"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd is not None, f'\"n_embd\" is not present, please provide valid inputs'\n",
        "    assert config.dropout is not None, f'\"dropout\" is not present, please provide valid inputs'\n",
        "\n",
        "    self.config = config\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(config.n_embd, 4*config.n_embd),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "        nn.Dropout(config.dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.net(x)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kPcJHzagq6J"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn_heads = MultiAttentionHead(config)\n",
        "    self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "    self.ffd = FeedForward(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.ln1(x)\n",
        "    x = self.attn_heads(out, out, out) + x\n",
        "    out = self.ln2(x)\n",
        "    out = self.ffd(out) + x\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASkXNyWXdphY"
      },
      "outputs": [],
      "source": [
        "class SequentialEncoder(nn.Sequential):\n",
        "  def forward(self, *inputs):\n",
        "    x, = inputs\n",
        "    for module in self._modules.values():\n",
        "      x = module(x)\n",
        "    return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.token_embedding_table = nn.Embedding(config.enc_vocab_size, config.n_embd)\n",
        "    self.pos_embedding_table = nn.Embedding(config.enc_vocab_size, config.n_embd)\n",
        "    self.encoderBlocks = SequentialEncoder(*[EncoderBlock(config) for _ in range(config.n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T = x.shape # (B, T)\n",
        "    tok_emb = self.token_embedding_table(x) # (B, T, C) C===n_embd\n",
        "    pos_emb = self.pos_embedding_table(x) # (B, T, C)\n",
        "    out = tok_emb + pos_emb # (B, T, C)\n",
        "    out = self.encoderBlocks(out) # (B, T, C)\n",
        "    out = self.ln_f(out)\n",
        "    return out # (B, T, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXHE4NsFKGhJ"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "    self.mask_attn_heads = MultiAttentionHead(config,True)\n",
        "    self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "    self.cross_attn_heads = MultiAttentionHead(config)\n",
        "    self.ln3 = nn.LayerNorm(config.n_embd)\n",
        "    self.ffd = FeedForward(config)\n",
        "    self\n",
        "\n",
        "  def forward(self, x, enc_out):\n",
        "    out = self.ln1(x)\n",
        "    x = self.mask_attn_heads(out, out, out) + x # (B, T, C)\n",
        "    out = self.ln2(x)\n",
        "    x = self.cross_attn_heads(enc_out, enc_out, out) + x # (B, T, C)\n",
        "    out = self.ln3(x)\n",
        "    out = self.ffd(out) + x # (B, T, C)\n",
        "    return out # (B, T, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQYGSHaDJcf7"
      },
      "outputs": [],
      "source": [
        "class SequentialDecoder(nn.Sequential):\n",
        "  def forward(self, *inputs):\n",
        "    x, enc_out = inputs\n",
        "    for module in self._modules.values():\n",
        "      x = module(x, enc_out)\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.token_embedding_table = nn.Embedding(config.dec_vocab_size, config.n_embd)\n",
        "    self.pos_embedding_table = nn.Embedding(config.dec_vocab_size, config.n_embd)\n",
        "    self.decoderBlocks = SequentialDecoder(*[DecoderBlock(config) for _ in range(config.n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.dec_vocab_size)\n",
        "\n",
        "  def forward(self, x, enc_out):\n",
        "    B, T = x.shape # (B, T)\n",
        "    tok_emb = self.token_embedding_table(x) # (B, T, C) C===n_embd\n",
        "    pos_emb = self.pos_embedding_table(x) # (B, T, C)\n",
        "    out = tok_emb + pos_emb # (B, T, C)\n",
        "    out = self.decoderBlocks(out, enc_out) # (B, T, C)\n",
        "    out = self.ln_f(out)\n",
        "    out = self.lm_head(out) # (B, T, dec_vocab_size)\n",
        "\n",
        "    return out # (B, T, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLd0O8Uo3tSE"
      },
      "outputs": [],
      "source": [
        "class TokenEncoding(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "  def forward(self, lang_sen_list, start = False, end = True):\n",
        "    if isinstance(lang_sen_list, str):\n",
        "      lang_sen_list = [lang_sen_list]\n",
        "\n",
        "    return torch.tensor([self.config.encoder(sentence,\n",
        "                         self.config.block_size,\n",
        "                         start,\n",
        "                         end) for sentence in lang_sen_list], dtype = torch.long)\n",
        "\n",
        "class TokenDecoding(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "  def forward(self, enc_sen_list, replace_token = None):\n",
        "    if replace_token is not None:\n",
        "      return [self.config.decoder(enc_list, replace_token) for enc_list in enc_sen_list]\n",
        "    return [self.config.decoder(enc_list) for enc_list in enc_sen_list]\n",
        "\n",
        "\n",
        "class SentenceTokenize():\n",
        "  def __init__(self, config):\n",
        "    assert config.sen_encoder is not None\n",
        "    assert config.sen_decoder is not None\n",
        "\n",
        "    self.config = config\n",
        "    self.encoder, self.decoder = generate_encoder_decoder(config.sen_encoder, config.sen_decoder)\n",
        "\n",
        "  def tokenize(sentence, key_to_index, max_sentence_length, start = True, end = True):\n",
        "    sentence_indicies = []\n",
        "    if start:\n",
        "      sentence_indicies += [key_to_index[START_TOKEN]]\n",
        "    sentence_indicies += [key_to_index[ch] for ch in list(sentence)]\n",
        "    if end:\n",
        "      sentence_indicies.append(key_to_index[END_TOKEN])\n",
        "    # Adding PADDING_TOKEN so that sentence_indicies array length is equivalent to max_sentence_length\n",
        "    if max_sentence_length is not None:\n",
        "      sentence_indicies += [key_to_index[PADDING_TOKEN]] * (max_sentence_length - len(sentence_indicies))\n",
        "    return sentence_indicies\n",
        "\n",
        "  def replace_token(ch, replaceTokens):\n",
        "    if ch in replaceTokens:\n",
        "      return replaceTokens[ch]\n",
        "    return ch\n",
        "\n",
        "  def generate_encoder_decoder(key_to_index, index_to_key):\n",
        "    replaceTokens = {START_TOKEN : \"\", PADDING_TOKEN : \"\", END_TOKEN : \"\"}\n",
        "\n",
        "    encoder = lambda s, max_sentence_length = None, start = True, end = True: tokenize(s.lower(), key_to_index, max_sentence_length, start, end)\n",
        "    decoder = lambda enc_list, replaceTokens = replaceTokens: ''.join([replace_token(index_to_key[i], replaceTokens) for i in enc_list])\n",
        "\n",
        "    return encoder, decoder\n",
        "\n",
        "  def sen_encode(self, lang_sen_list, start = False, end = True):\n",
        "    if isinstance(lang_sen_list, str):\n",
        "      lang_sen_list = [lang_sen_list]\n",
        "\n",
        "    return torch.tensor([self.encoder(sentence,\n",
        "                         self.config.block_size,\n",
        "                         start,\n",
        "                         end) for sentence in lang_sen_list], dtype = torch.long)\n",
        "\n",
        "  def sen_decode(self, enc_sen_list, replace_token = None):\n",
        "    if replace_token is not None:\n",
        "      return [self.decoder(enc_list, replace_token) for enc_list in enc_sen_list]\n",
        "    return [self.decoder(enc_list) for enc_list in enc_sen_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cWzCoGpNTD7"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, model_config, token_configs):\n",
        "    super().__init__()\n",
        "    assert len(token_configs) == 2, f'token_configs should two config classes, one for source language conversions, one for target language conversions'\n",
        "    self.config = model_config\n",
        "    self.encoder = Encoder(model_config)\n",
        "    self.decoder = Decoder(model_config)\n",
        "\n",
        "    self.token_configs = token_configs\n",
        "    src_token_config, tar_token_config = token_configs\n",
        "    self.src_tokenize = SentenceTokenize(src_token_config)\n",
        "    self.tar_tokenize = SentenceTokenize(tar_token_config)\n",
        "\n",
        "    # self.token_configs = token_configs\n",
        "    # srclang_token_config, tarlang_token_config = token_configs\n",
        "    # self.srclang_token_encoder = TokenEncoding(srclang_token_config)\n",
        "    # self.srclang_token_decoder = TokenDecoding(srclang_token_config)\n",
        "    # self.tarlang_token_encoder = TokenEncoding(tarlang_token_config)\n",
        "    # self.tarlang_token_decoder = TokenDecoding(tarlang_token_config)\n",
        "\n",
        "    self.initial()\n",
        "\n",
        "  def initial(self):\n",
        "    for params in self.parameters():\n",
        "      if params.dim() > 1:\n",
        "          nn.init.xavier_uniform_(params)\n",
        "\n",
        "  def forward(self, enc_inp, dec_inp, targets = None):\n",
        "    enc_inp = self.src_tokenize.sen_encode(enc_inp, False, False).to(self.config.device) # (B, T)\n",
        "    enc_out = self.encoder(enc_inp) # (B, T, C) C===n_embd\n",
        "\n",
        "    dec_inp = self.tar_tokenize.sen_encode(dec_inp, True, True).to(self.config.device) # (B, T)\n",
        "    dec_inp = dec_inp.to(self.config.device)\n",
        "    logits = self.decoder(dec_inp, enc_out) # (B, T, dec_vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      targets = self.tar_tokenize.sen_encode(targets, False, True).to(self.config.device) # (B, T)\n",
        "      B, T, V = logits.shape\n",
        "      logits = logits.view(B * T, V) # (B * T, V)\n",
        "      targets = targets.view(B * T) # (B * T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self, enc_inp):\n",
        "    self.eval()\n",
        "    flag = False\n",
        "    if isinstance(enc_inp, str):\n",
        "      flag = True\n",
        "      enc_inp = [enc_inp]\n",
        "\n",
        "    B = len(enc_inp)\n",
        "    outputs = []\n",
        "\n",
        "    for batch_num in range(B):\n",
        "      dec_inp = [\"\"]\n",
        "      for i in range(max_sequence_length - 1):\n",
        "        logits, _ = self(enc_inp, dec_inp)\n",
        "        next_token_index = torch.argmax(logits, dim = -1)\n",
        "        next_token_index = next_token_index[:, i].view(-1, 1)\n",
        "        next_token = self.tar_tokenize.sen_decode(next_token_index.tolist(),{})\n",
        "        if next_token[0] in [START_TOKEN, END_TOKEN, PADDING_TOKEN]:\n",
        "          break\n",
        "        dec_inp = [sen + nxt_t for (sen, nxt_t) in zip(dec_inp, next_token)]\n",
        "      outputs.append(dec_inp[0])\n",
        "    if flag: return outputs[0]\n",
        "    return outputs\n",
        "\n",
        "  def save_model(self, filepath, losses = None, config = None):\n",
        "    if config is None:\n",
        "      config = self.config\n",
        "    torch.save({\n",
        "        \"state_dict\": self.state_dict(),\n",
        "        \"config\": self.config,\n",
        "        \"token_configs\": self.token_configs,\n",
        "        \"losses\": losses\n",
        "    }, filepath)\n",
        "\n",
        "  def load_model(filepath, device = None):\n",
        "    # Load the model from the buffer\n",
        "    checkpoint = torch.load(filepath)\n",
        "    config = checkpoint['config']\n",
        "    token_configs = checkpoint['token_configs']\n",
        "    if device is not None:\n",
        "      config['device'] = device\n",
        "    model = Transformer(config, token_configs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0Sfutw8n584"
      },
      "source": [
        "## Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cst-KTJnK56X"
      },
      "outputs": [],
      "source": [
        "# Training HyperParameters\n",
        "learning_rate = 3e-4\n",
        "max_iters = 10\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_interval = 1000\n",
        "eval_iters = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIQB9gAteJQ_"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "  def __init__(self, **kwargs):\n",
        "     for key, value in kwargs.items():\n",
        "      setattr(self, key, value)\n",
        "\n",
        "model_config = Config(\n",
        "    batch_size = 32,\n",
        "    block_size = max_sequence_length,\n",
        "    enc_vocab_size = len(english_vocabulary),\n",
        "    dec_vocab_size = len(telugu_vocabulary),\n",
        "\n",
        "    n_embd = 512,\n",
        "    n_layer = 2,\n",
        "    n_head = 8,\n",
        "    head_size = 512//8,\n",
        "    dropout = 0.2,\n",
        "\n",
        "    learning_rate = learning_rate,\n",
        "    max_iter = max_iters,\n",
        "    device = device,\n",
        "    eval_interval = eval_interval,\n",
        "    eval_iters = eval_iters\n",
        ")\n",
        "\n",
        "en_token_config = Config(\n",
        "    block_size = model_config.block_size,\n",
        "    sen_encoder = english_to_index,\n",
        "    sen_decoder = index_to_english\n",
        ")\n",
        "\n",
        "te_token_config = Config(\n",
        "    block_size = model_config.block_size,\n",
        "    sen_encoder = telugu_to_index,\n",
        "    sen_decoder = index_to_telugu\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wpRsR-8oDqL"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj0WgDgYA9LK"
      },
      "outputs": [],
      "source": [
        "def get_DataLoader(dataset, batch_size):\n",
        "  dataLoader = DataLoader(dataset, batch_size, shuffle = True)\n",
        "  iterator = iter(dataLoader)\n",
        "  return dataLoader, iterator\n",
        "\n",
        "trainDataLoader, trainIterator = get_DataLoader(en_te_dataset['train'], model_config.batch_size)\n",
        "valDataLoader, valIterator = get_DataLoader(en_te_dataset['val'], batch_size = model_config.batch_size)\n",
        "testDataLoader, testIterator = get_DataLoader(en_te_dataset['test'], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5nwSkVJy2L-"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(model_config, (en_token_config, te_token_config))\n",
        "transformer.to(device)\n",
        "optimizer = torch.optim.AdamW(transformer.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1Z7WPMF-ROD",
        "outputId": "b50569fc-0f4f-4f20-a5d7-c9606a214bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters of the model: 14.974587M parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'number of parameters of the model: {num_parameters(sum(p.numel() for p in transformer.parameters()))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnL8RW1um3Q6"
      },
      "outputs": [],
      "source": [
        "# def get_batch(split, batch_size):\n",
        "#   assert split in en_te_dataset.keys(), f'Provide valid split which should be [\"train\", \"val\", \"test\"]'\n",
        "\n",
        "#   enc_data = []\n",
        "#   dec_data = []\n",
        "#   ix = torch.randint(len(data), (batch_size,))\n",
        "#   for i in ix:\n",
        "#     enc_sen, dec_sen = en_te_dataset[split][i.item()]\n",
        "#     enc_data.append(enc_sen)\n",
        "#     dec_data.append(dec_sen)\n",
        "#   return enc_data, dec_data\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def estimate_loss(model, config):\n",
        "#   out = {}\n",
        "#   model.eval()\n",
        "#   for split in ['train', 'val']:\n",
        "#     losses = torch.zeros(eval_iters)\n",
        "#     for k in range(config.eval_iters):\n",
        "#       en_batch, te_batch = get_batch(split, config.batch_size)\n",
        "#       logits, loss = transformer(en_batch, te_batch, te_batch)\n",
        "#       losses[k] = (loss.item())\n",
        "#     out[split] = losses.mean()\n",
        "#   return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "SxXUfLoQAWeK",
        "outputId": "8b41db4e-953c-4ea8-92e6-4b65aaed3f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 Train loss: 3.8702831268310547, Val loss: 3.863093852996826, time taken: 1.3994348049163818\n",
            "English: I love you\n",
            "Telugu Prediction: \n",
            "Telugu Target: నేను నిన్ను ప్రేమిస్తున్నాను\n",
            "------------------------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 5.06 MiB is free. Process 80915 has 14.74 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 555.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-ed89f6961af9>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0men_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-1d06fc4eccfc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, enc_inp, dec_inp, targets)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0menc_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_tokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msen_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0menc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_inp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, C) C===n_embd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdec_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtar_tokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msen_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-9d5aadbfd6c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m \u001b[0;31m# (B, T, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoderBlocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m \u001b[0;31m# (B, T, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-9d5aadbfd6c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-b38806e2774e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-1ddf20e392a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_heads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, n_head * head_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, n_embd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-1ddf20e392a7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_heads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, n_head * head_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, n_embd)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-1ddf20e392a7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, head_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, head_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, T, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 5.06 MiB is free. Process 80915 has 14.74 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 555.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "losses = {'train':[],\n",
        "          'val': []}\n",
        "for step in range(max_iters):\n",
        "  out = {'train':[],\n",
        "          'val': []}\n",
        "  for batch_num, batch in enumerate(trainIterator):\n",
        "    transformer.train()\n",
        "    en_batch, te_batch = batch\n",
        "    logits, loss = transformer(en_batch, te_batch, te_batch)\n",
        "    out['train'].append(loss)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    transformer.eval()\n",
        "    for val_batch in valDataLoader:\n",
        "      en_val_batch, te_val_batch = val_batch\n",
        "      break\n",
        "    logits, loss = transformer(en_val_batch, te_val_batch, te_val_batch)\n",
        "    out['val'].append(loss)\n",
        "\n",
        "    if batch_num % model_config.eval_interval == 0:\n",
        "      for split in losses.keys():\n",
        "        losses[split].append(torch.tensor(out[split]).mean().item())\n",
        "        out[split] = []\n",
        "\n",
        "      print(f'Iteration {batch_num} Train loss: {losses[\"train\"][-1]}, Val loss: {losses[\"val\"][-1]}, time taken: {time.time() - start_time}')\n",
        "      en_sentence = \"I love you\"\n",
        "      te_sentence = \"నేను నిన్ను ప్రేమిస్తున్నాను\"\n",
        "      print(f'English: {en_sentence}')\n",
        "      print(f'Telugu Prediction: {transformer.generate(en_sentence)}')\n",
        "      print(f'Telugu Target: {te_sentence}')\n",
        "      print(f'------------------------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "  for split in losses.keys():\n",
        "    losses[split].append(torch.tensor(out[split]).mean().item())\n",
        "\n",
        "  print(f'***************************************************************************************************************************')\n",
        "  print(f'Epoch {step} Train loss: {losses[\"train\"][-1]}, Val loss: {losses[\"val\"][-1]}, time taken: {time.time() - start_time}')\n",
        "  en_sentence = \"I love you\"\n",
        "  te_sentence = \"నేను నిన్ను ప్రేమిస్తున్నాను\"\n",
        "  print(f'English: {en_sentence}')\n",
        "  print(f'Telugu Prediction: {transformer.generate(en_sentence)}')\n",
        "  print(f'Telugu Target: {te_sentence}')\n",
        "  print(f'***************************************************************************************************************************')\n",
        "print(f'time execution: {format_time(time.time() - start_time)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CELjdeIeStub",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f8317199-b1fe-4c8e-9182-890e153f0931"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ూఝృదఘద దఇ౪ా ఫాఝా చదఉ\"దఝదఝఇ\"ఇషఇఇ\\'ష\"జ|షషఝష\"ఝషఇఝఝఝ>౧ఇష\"ఝఇ(ఇ౯ .\"|జషూజ7ఙజష\"\"ష\"\"\"\"\"షఝజజఝష<ౄఝ\"నఝూఝఝఝఝప\"చ/చఝచచచఝ\",\\'\"ఇచఐచఝఙచ,చ*చఝఙ`ఝఋచఐష|\"\\'చచఐ\"ౠఇ\\'ఐషఝఐచేచఐ\\'`నఐఐ\\'౭$`ృపఐచఐ౭ఝ*ఐ\"9౭ఐష\\'ఐౠఋఝఐచఉఐఐ\\'ఐ౭ఐచఐ\\'౭ఐ\\'౭ఐషఐఐ\\'ఐఐచఉఐ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 158
        }
      ],
      "source": [
        "filepath = './save_model.pt'\n",
        "transformer = Transformer(model_config, (en_token_config, te_token_config))\n",
        "# transformer.save_model(filepath)\n",
        "# transformer = Transformer.load_model(filepath)\n",
        "lang1_inp = 'Rohit'\n",
        "transformer.generate(lang1_inp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jnvM-Zrq5sb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "04F2mIUHJeDB",
        "osPIT_7rr5qT",
        "bc18Fd0bLdwv",
        "OOudE0GyESR7",
        "Zh0qMaDRK1DF",
        "v0Sfutw8n584"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}